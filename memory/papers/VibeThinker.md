# VibeThinker-1.5B Paper

## Title
Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B

## Basic Info
- **Published**: November 8, 2025
- **Paper ID**: 2511.06221
- **Status**: #2 Paper of the day (DenseHub, Nov 12)
- **Source**: arXiv

## Authors
Sen Xu, Yi Zhou, Wei Wang, Jixin Min, Zhibin Yin, Yingwei Dai, Shixi Liu, Lianyu Pang, Yirong Chen, Junlin Zhang
(Sina Weibo Inc.)

## URL
- **Paper**: https://arxiv.org/abs/2511.06221
- **GitHub**: https://github.com/WeiboAI/VibeThinker
- **Hugging Face**: https://huggingface.co/WeiboAI/VibeThinker-1.5B
- **License**: MIT

## BibTeX Citation
```bibtex
@misc{xu2025tinymodelbiglogic,
  title={Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B},
  author={Sen Xu and Yi Zhou and Wei Wang and Jixin Min and Zhibin Yin and Yingwei Dai and Shixi Liu and Lianyu Pang and Yirong Chen and Junlin Zhang},
  year={2025},
  eprint={2511.06221},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2511.06221}
}
```

## Key Contributions
- **VibeThinker-1.5B**: A 1.5B-parameter dense model with superior reasoning capabilities
- **Spectrum-to-Signal Principle (SSP)**: Framework combining diversity-driven optimization with policy optimization
- **Two-Stage approach**:
  1. Two-Stage Diversity-Exploring Distillation (SFT)
  2. MaxEnt-Guided Policy Optimization (RL)
- **Training Cost**: Only $7,800
- **Performance**: Outperforms DeepSeek R1 (400x larger), surpasses Magistral Medium and Claude Opus 4 on benchmarks
- **Benchmarks**: AIME24 (80.3 vs 79.8), AIME25 (74.4 vs 70.0), HMMT25 (50.4 vs 41.7), LiveCodeBench V6 (51.1 vs 50.3)

## Status
- [ ] Reviewed
- [ ] Notes added

## Related Papers
- DeepSeek R1
- Kimi k2
- GPT OSS-20B Medium
- Magistral Medium
- Claude Opus 4

