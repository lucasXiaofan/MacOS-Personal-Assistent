# Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm

## Paper Info
- **URL**: https://arxiv.org/abs/2511.04570
- **Published**: November 6, 2025
- **Submitted by**: Tony.Li on Nov 7
- **Status**: #1 Paper of the day (OpenMOSS)

## BibTeX Citation
```bibtex
@misc{tong2025thinkingvideovideogeneration,
  title={Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm},
  author={Jingqi Tong and Yurong Mou and Hangcheng Li and Mingzhe Li and Yongzhuo Yang and Ming Zhang and Qiguang Chen and Tianyi Liang and Xiaomeng Hu and Yining Zheng and Xinchi Chen and Jun Zhao and Xuanjing Huang and Xipeng Qiu},
  year={2025},
  eprint={2511.04570},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2511.04570}
}
```

## Authors
Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu

## GitHub Repository
✅ Found: https://github.com/tongjingqi/Thinking-with-Video

## Key Concept
The paper introduces "Thinking with Video", a new paradigm leveraging video generation for multimodal reasoning. It proposes that video generation models can serve as unified multimodal understanding and generation models.

## Main Contributions
1. **VideoThinkBench**: A benchmark for evaluating video-based reasoning
   - Vision-centric tasks (e.g., Eyeballing Puzzles)
   - Text-centric tasks (e.g., subsets of GSM8K, MMMU)

2. **Sora-2 Evaluation**: Demonstrates that Sora-2 can:
   - Surpass state-of-the-art VLMs on several vision-centric tasks
   - Achieve 92% accuracy on MATH
   - Reach 75.53% accuracy on MMMU

## Abstract Summary
The "Thinking with Video" paradigm enhances multimodal reasoning by integrating video generation models, addressing limitations of "Thinking with Text" and "Thinking with Images" paradigms:
- **Problem**: Images capture only single moments; text and vision are separated modalities
- **Solution**: Use video generation models like Sora-2 to bridge visual and textual reasoning in a unified temporal framework

## Relevance to LLM Agent Development
- Proposes video generation as a unified reasoning paradigm
- Relevant for multimodal reasoning in intelligent agents
- Shows potential for dynamic reasoning and temporal understanding

## Review Status
⏳ Waiting for user review - created 2025-11-13
